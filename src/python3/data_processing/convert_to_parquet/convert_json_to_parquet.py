from pyspark import SparkContext
from pyspark.sql import SparkSession, SQLContext
import time
import argparse
from calendar import monthrange
from convert_to_parquet import convert_to_parquet

'''
Convert JSON files to parquet for the given month and year,
and store it in the given S3 bucket

@param post_year             year of the given posts (e.g. 2019)
@param post_month            month of the given posts (1 for January and 12 for December)
@param json_s3_bucket        the url of the AWS S3 bucket storing JSON files (e.g "s3a://my-parquet-bucket/")
@param parquet_s3_bucket     the url of the AWS S3 bucket storing parquet files (e.g "s3a://my-json-bucket/")
@param logfile               the logfile to which to write information
'''

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    required = parser.add_argument_group('required arguments')
    required.add_argument("--year", help="post year (e.g. 2019)",
                          required=True)
    required.add_argument("--month", help="post month (e.g. 1 for January" +
                           " and 12 for December",  required=True)
    optional = parser.add_argument_group("optional arguments")
    optional.add_argument("--json_s3_bucket", help="url for the AWS S3 bucket holding JSON files." +
                          "This program assumes that the user is using parquet" +
                          " files generated by the convert_to_parquet.py utility " +
                          "script. The naming convention for that script is " +
                          "<s3_bucket>/posts/<year>-<month>/<year>-<month>-<day>" +
                          ".parquet e.g. 's3a://my-json-bucket/2019-12/2019-12-01.parquet'",
                          default = "s3a://reddit-data-json/")
    optional.add_argument("--parquet_s3_bucket", help="url for the AWS S3 bucket holding parquet files." +
                          "This program assumes that the user is using parquet" +
                          " files generated by the convert_to_parquet.py utility " +
                          "script. The naming convention for that script is " +
                          "<s3_bucket>/posts/<year>-<month>/<year>-<month>-<day>" +
                          ".parquet e.g. 's3a://my-parquet-bucket/2019-12/2019-12-01.parquet'",
                          default = "s3a://reddit-data-parquet/")
    optional.add_argument("--logfile_name", help="name of the logfile to which to" +
                          " write", default="logs/process_montly_data_2019_12.log")
    optional.add_argument("--day", help="day on which to start converting files for the month", default = 1)
    
    args =  parser.parse_args()
    
    sc = SparkContext("spark://ec2-3-219-180-255.compute-1.amazonaws.com:7077","convert_to_parquet")
    sqlContext = SQLContext(sc)
    post_year = int(args.year)
    post_month = int(args.month)
    post_day = int(args.day)
    
    #get number of days in month
    first_day_weekday, number_days_month = monthrange(post_year, post_month)
    daily_post_files = ["posts/" + str(post_year) + "-" + str(post_month).zfill(2) + "/RS_" + str(post_year) + '-' + str(post_month).zfill(2) +'-' + str(i).zfill(2) for i in range(post_day, number_days_month + 1)]
    logfile_name = args.logfile_name
    with open(logfile_name, "a") as logfile:
        for daily_post_file in daily_post_files:
            
            logfile.write("converting file  \"" + daily_post_file + "\" to parquet\n")
            time_start = time.time()
            convert_to_parquet(sqlContext, daily_post_file, logfile, args.json_s3_bucket, args.parquet_s3_bucket)
            time_end = time.time()
            logfile.write("time elapsed: {} \n\n".format(time_end - time_start))
